#!/usr/bin/env python3
"""
SAMURAI ONNX è§†é¢‘è¿½è¸ªæ¼”ç¤ºè„šæœ¬

è¿™ä¸ªè„šæœ¬æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„è§†é¢‘ç›®æ ‡è¿½è¸ªæ¼”ç¤ºï¼Œæ”¯æŒï¼š
- æŒ‡å®šè¾“å…¥è§†é¢‘è·¯å¾„
- æŒ‡å®šè¾“å‡ºè§†é¢‘è·¯å¾„
- æŒ‡å®šåˆå§‹è¾¹ç•Œæ¡†
- å®æ—¶è¿›åº¦æ˜¾ç¤º
- è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡

ä½¿ç”¨æ–¹æ³•:
    python video_tracking_demo.py --input video.mp4 --output result.mp4 --bbox "100,100,200,150"
    python video_tracking_demo.py --input video.mp4 --bbox "100,100,200,150"  # åªä¿å­˜ç»“æœæ–‡ä»¶
    python video_tracking_demo.py --help  # æŸ¥çœ‹æ‰€æœ‰é€‰é¡¹

ä¾èµ–:
    - onnxruntime
    - opencv-python
    - numpy
"""

import os
import sys
import cv2
import numpy as np
import argparse
import time
import json
from pathlib import Path
from typing import Tuple, List, Optional, Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

try:
    from scripts.onnx_inference import SAMURAIONNXPredictor
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥ SAMURAIONNXPredictor")
    print("è¯·ç¡®ä¿ scripts/onnx_inference.py æ–‡ä»¶å­˜åœ¨")
    sys.exit(1)

class VideoTrackingDemo:
    """
    SAMURAI ONNX è§†é¢‘è¿½è¸ªæ¼”ç¤ºç±»
    
    æä¾›å®Œæ•´çš„è§†é¢‘ç›®æ ‡è¿½è¸ªåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - è§†é¢‘è¯»å–å’Œå¤„ç†
    - ç›®æ ‡è¿½è¸ª
    - ç»“æœå¯è§†åŒ–
    - æ€§èƒ½ç»Ÿè®¡
    """
    
    def __init__(self, model_dir: str = "onnx_models", device: str = "cpu"):
        """
        åˆå§‹åŒ–è§†é¢‘è¿½è¸ªæ¼”ç¤º
        
        Args:
            model_dir: ONNXæ¨¡å‹æ–‡ä»¶ç›®å½•
            device: æ¨ç†è®¾å¤‡ ("cpu" æˆ– "cuda")
        """
        self.model_dir = model_dir
        self.device = device
        
        # åˆå§‹åŒ–è¿½è¸ªå™¨
        print("ğŸš€ åˆå§‹åŒ– SAMURAI ONNX è¿½è¸ªå™¨...")
        try:
            self.predictor = SAMURAIONNXPredictor(model_dir, device)
            print("âœ… è¿½è¸ªå™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ è¿½è¸ªå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_frames': 0,
            'processing_time': 0,
            'avg_fps': 0,
            'bbox_changes': 0,
            'confidence_scores': []
        }
    
    def validate_inputs(self, input_path: str, bbox: Tuple[int, int, int, int]) -> bool:
        """
        éªŒè¯è¾“å…¥å‚æ•°
        
        Args:
            input_path: è¾“å…¥è§†é¢‘è·¯å¾„
            bbox: åˆå§‹è¾¹ç•Œæ¡† (x, y, w, h)
            
        Returns:
            éªŒè¯æ˜¯å¦é€šè¿‡
        """
        # æ£€æŸ¥è§†é¢‘æ–‡ä»¶
        if not os.path.exists(input_path):
            print(f"âŒ è¾“å…¥è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
            return False
        
        # æ£€æŸ¥è§†é¢‘æ ¼å¼
        cap = cv2.VideoCapture(input_path)
        if not cap.isOpened():
            print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {input_path}")
            return False
        
        # è·å–è§†é¢‘ä¿¡æ¯
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"ğŸ“¹ è§†é¢‘ä¿¡æ¯:")
        print(f"   åˆ†è¾¨ç‡: {width}x{height}")
        print(f"   å¸§ç‡: {fps:.1f} fps")
        print(f"   æ€»å¸§æ•°: {frame_count}")
        
        # æ£€æŸ¥è¾¹ç•Œæ¡†
        x, y, w, h = bbox
        if x < 0 or y < 0 or w <= 0 or h <= 0:
            print(f"âŒ æ— æ•ˆçš„è¾¹ç•Œæ¡†: {bbox}")
            return False
        
        if x + w > width or y + h > height:
            print(f"âŒ è¾¹ç•Œæ¡†è¶…å‡ºè§†é¢‘èŒƒå›´: {bbox} (è§†é¢‘å°ºå¯¸: {width}x{height})")
            return False
        
        print(f"ğŸ¯ åˆå§‹è¾¹ç•Œæ¡†: ({x}, {y}, {w}, {h})")
        
        cap.release()
        return True
    
    def track_video(self, input_path: str, initial_bbox: Tuple[int, int, int, int],
                   output_path: Optional[str] = None, save_results: bool = True) -> Dict[str, Any]:
        """
        æ‰§è¡Œè§†é¢‘è¿½è¸ª
        
        Args:
            input_path: è¾“å…¥è§†é¢‘è·¯å¾„
            initial_bbox: åˆå§‹è¾¹ç•Œæ¡† (x, y, w, h)
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            save_results: æ˜¯å¦ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
            
        Returns:
            è¿½è¸ªç»“æœå­—å…¸
        """
        # éªŒè¯è¾“å…¥
        if not self.validate_inputs(input_path, initial_bbox):
            raise ValueError("è¾“å…¥éªŒè¯å¤±è´¥")
        
        # æ‰“å¼€è§†é¢‘
        cap = cv2.VideoCapture(input_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # åˆå§‹åŒ–è¾“å‡ºè§†é¢‘å†™å…¥å™¨
        writer = None
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            print(f"ğŸ“ å°†ä¿å­˜è¾“å‡ºè§†é¢‘åˆ°: {output_path}")
        
        # è¿½è¸ªç»“æœ
        tracking_results = []
        confidence_scores = []
        
        # å¼€å§‹è¿½è¸ª
        print(f"\nğŸ¬ å¼€å§‹è¿½è¸ªè§†é¢‘...")
        start_time = time.time()
        
        frame_idx = 0
        prev_bbox = initial_bbox
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            try:
                # æ‰§è¡Œè¿½è¸ª
                frame_start = time.time()
                mask, confidence, memory_features = self.predictor.predict_mask(frame, initial_bbox)
                frame_time = time.time() - frame_start
                
                # ä»æ©ç æ›´æ–°è¾¹ç•Œæ¡†
                if mask.any():
                    y_indices, x_indices = np.where(mask)
                    if len(x_indices) > 0 and len(y_indices) > 0:
                        x1, x2 = x_indices.min(), x_indices.max()
                        y1, y2 = y_indices.min(), y_indices.max()
                        
                        # æ£€æŸ¥æ©ç èŒƒå›´æ˜¯å¦è¿‡å¤§
                        mask_area = np.sum(mask)
                        image_area = frame.shape[0] * frame.shape[1]
                        
                        if mask_area > image_area * 0.5:  # å¦‚æœæ©ç è¶…è¿‡50%çš„å›¾åƒ
                            print(f"å¸§ {frame_idx+1}: æ©ç è¿‡å¤§ ({mask_area}/{image_area}), ä¿æŒåŸè¾¹ç•Œæ¡†")
                            current_bbox = prev_bbox
                        else:
                            # æ·»åŠ è¾¹è·
                            padding = 5
                            x1 = max(0, x1 - padding)
                            y1 = max(0, y1 - padding)
                            x2 = min(width - 1, x2 + padding)
                            y2 = min(height - 1, y2 + padding)
                            
                            current_bbox = (x1, y1, x2 - x1, y2 - y1)
                            
                            # æ£€æŸ¥è¾¹ç•Œæ¡†å˜åŒ–
                            if current_bbox != prev_bbox:
                                self.stats['bbox_changes'] += 1
                                prev_bbox = current_bbox
                    else:
                        current_bbox = prev_bbox
                else:
                    current_bbox = prev_bbox
                
                # ä¿å­˜ç»“æœ
                tracking_results.append(current_bbox)
                confidence_scores.append(confidence)
                
                # ç»˜åˆ¶ç»“æœ
                if writer:
                    self._draw_tracking_info(frame, current_bbox, confidence, frame_idx, frame_time)
                    writer.write(frame)
                
                frame_idx += 1
                
                # è¿›åº¦æ˜¾ç¤º
                if frame_idx % 30 == 0 or frame_idx == total_frames:
                    progress = frame_idx / total_frames * 100
                    elapsed = time.time() - start_time
                    current_fps = frame_idx / elapsed
                    print(f"   è¿›åº¦: {frame_idx}/{total_frames} ({progress:.1f}%) - {current_fps:.2f} fps")
                
            except Exception as e:
                print(f"âš ï¸  å¸§ {frame_idx} å¤„ç†å¤±è´¥: {e}")
                # ä½¿ç”¨ä¸Šä¸€å¸§çš„ç»“æœ
                if tracking_results:
                    tracking_results.append(tracking_results[-1])
                    confidence_scores.append(confidence_scores[-1])
                else:
                    tracking_results.append(initial_bbox)
                    confidence_scores.append(0.0)
                frame_idx += 1
        
        # æ¸…ç†èµ„æº
        cap.release()
        if writer:
            writer.release()
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_time = time.time() - start_time
        self.stats.update({
            'total_frames': len(tracking_results),
            'processing_time': total_time,
            'avg_fps': len(tracking_results) / total_time,
            'confidence_scores': confidence_scores
        })
        
        # ä¿å­˜ç»“æœ
        results = {
            'tracking_results': tracking_results,
            'confidence_scores': confidence_scores,
            'stats': self.stats,
            'video_info': {
                'width': width,
                'height': height,
                'fps': fps,
                'total_frames': total_frames
            }
        }
        
        if save_results:
            self._save_results(input_path, results)
        
        return results
    
    def _draw_tracking_info(self, frame: np.ndarray, bbox: Tuple[int, int, int, int],
                           confidence: float, frame_idx: int, frame_time: float):
        """
        åœ¨å¸§ä¸Šç»˜åˆ¶è¿½è¸ªä¿¡æ¯
        
        Args:
            frame: è¾“å…¥å¸§
            bbox: è¾¹ç•Œæ¡† (x, y, w, h)
            confidence: ç½®ä¿¡åº¦
            frame_idx: å¸§ç´¢å¼•
            frame_time: å¤„ç†æ—¶é—´
        """
        x, y, w, h = bbox
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        color = (0, 255, 0) if confidence > 0.5 else (0, 255, 255)
        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
        
        # ç»˜åˆ¶ä¿¡æ¯æ–‡æœ¬
        info_text = [
            f"Frame: {frame_idx}",
            f"Conf: {confidence:.3f}",
            f"Time: {frame_time*1000:.1f}ms",
            f"BBox: ({x},{y},{w},{h})"
        ]
        
        for i, text in enumerate(info_text):
            cv2.putText(frame, text, (10, 30 + i * 25), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
    
    def _save_results(self, input_path: str, results: Dict[str, Any]):
        """
        ä¿å­˜è¿½è¸ªç»“æœåˆ°æ–‡ä»¶
        
        Args:
            input_path: è¾“å…¥è§†é¢‘è·¯å¾„
            results: è¿½è¸ªç»“æœ
        """
        base_name = Path(input_path).stem
        
        # ä¿å­˜è¾¹ç•Œæ¡†ç»“æœ
        bbox_file = f"{base_name}_tracking_results.txt"
        with open(bbox_file, 'w') as f:
            for bbox in results['tracking_results']:
                f.write(f"{bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]}\n")
        print(f"ğŸ“„ è¾¹ç•Œæ¡†ç»“æœä¿å­˜åˆ°: {bbox_file}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        json_file = f"{base_name}_tracking_details.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“Š è¯¦ç»†ç»“æœä¿å­˜åˆ°: {json_file}")
    
    def print_statistics(self, results: Dict[str, Any]):
        """
        æ‰“å°è¿½è¸ªç»Ÿè®¡ä¿¡æ¯
        
        Args:
            results: è¿½è¸ªç»“æœ
        """
        stats = results['stats']
        video_info = results['video_info']
        
        print(f"\nğŸ“Š è¿½è¸ªç»Ÿè®¡ä¿¡æ¯:")
        print(f"=" * 40)
        print(f"æ€»å¸§æ•°: {stats['total_frames']}")
        print(f"å¤„ç†æ—¶é—´: {stats['processing_time']:.2f}ç§’")
        print(f"å¹³å‡é€Ÿåº¦: {stats['avg_fps']:.2f} fps")
        print(f"è¾¹ç•Œæ¡†å˜åŒ–æ¬¡æ•°: {stats['bbox_changes']}")
        
        if stats['confidence_scores']:
            avg_conf = np.mean(stats['confidence_scores'])
            min_conf = np.min(stats['confidence_scores'])
            max_conf = np.max(stats['confidence_scores'])
            print(f"ç½®ä¿¡åº¦ç»Ÿè®¡:")
            print(f"  å¹³å‡: {avg_conf:.3f}")
            print(f"  æœ€å°: {min_conf:.3f}")
            print(f"  æœ€å¤§: {max_conf:.3f}")
        
        print(f"\nè§†é¢‘ä¿¡æ¯:")
        print(f"  åˆ†è¾¨ç‡: {video_info['width']}x{video_info['height']}")
        print(f"  å¸§ç‡: {video_info['fps']:.1f} fps")
        print(f"  æ€»å¸§æ•°: {video_info['total_frames']}")

def parse_bbox(bbox_str: str) -> Tuple[int, int, int, int]:
    """
    è§£æè¾¹ç•Œæ¡†å­—ç¬¦ä¸²
    
    Args:
        bbox_str: è¾¹ç•Œæ¡†å­—ç¬¦ä¸² "x,y,w,h"
        
    Returns:
        è¾¹ç•Œæ¡†å…ƒç»„ (x, y, w, h)
    """
    try:
        parts = bbox_str.split(',')
        if len(parts) != 4:
            raise ValueError("è¾¹ç•Œæ¡†æ ¼å¼é”™è¯¯")
        
        bbox = tuple(map(int, parts))
        if any(x < 0 for x in bbox):
            raise ValueError("è¾¹ç•Œæ¡†å€¼ä¸èƒ½ä¸ºè´Ÿæ•°")
        
        return bbox
    except Exception as e:
        raise ValueError(f"è¾¹ç•Œæ¡†è§£æå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="SAMURAI ONNX è§†é¢‘è¿½è¸ªæ¼”ç¤º",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åŸºæœ¬è¿½è¸ª
  python video_tracking_demo.py --input video.mp4 --bbox "100,100,200,150"
  
  # ä¿å­˜è¾“å‡ºè§†é¢‘
  python video_tracking_demo.py --input video.mp4 --output result.mp4 --bbox "100,100,200,150"
  
  # ä½¿ç”¨GPUåŠ é€Ÿ
  python video_tracking_demo.py --input video.mp4 --bbox "100,100,200,150" --device cuda
  
  # æŒ‡å®šæ¨¡å‹ç›®å½•
  python video_tracking_demo.py --input video.mp4 --bbox "100,100,200,150" --model_dir custom_models
        """
    )
    
    parser.add_argument("--input", "-i", required=True, 
                       help="è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--bbox", "-b", required=True,
                       help="åˆå§‹è¾¹ç•Œæ¡†ï¼Œæ ¼å¼: 'x,y,w,h'")
    parser.add_argument("--output", "-o",
                       help="è¾“å‡ºè§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--model_dir", "-m", default="onnx_models",
                       help="ONNXæ¨¡å‹æ–‡ä»¶ç›®å½• (é»˜è®¤: onnx_models)")
    parser.add_argument("--device", "-d", default="cpu", choices=["cpu", "cuda"],
                       help="æ¨ç†è®¾å¤‡ (é»˜è®¤: cpu)")
    parser.add_argument("--no_save", action="store_true",
                       help="ä¸ä¿å­˜ç»“æœæ–‡ä»¶")
    
    args = parser.parse_args()
    
    try:
        # è§£æè¾¹ç•Œæ¡†
        initial_bbox = parse_bbox(args.bbox)
        
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
        if not os.path.exists(args.input):
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
            return 1
        
        # æ£€æŸ¥æ¨¡å‹ç›®å½•
        if not os.path.exists(args.model_dir):
            print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {args.model_dir}")
            print("è¯·ç¡®ä¿ONNXæ¨¡å‹æ–‡ä»¶å·²æ­£ç¡®å¯¼å‡º")
            return 1
        
        # åˆ›å»ºæ¼”ç¤ºå®ä¾‹
        demo = VideoTrackingDemo(args.model_dir, args.device)
        
        # æ‰§è¡Œè¿½è¸ª
        print(f"ğŸ¯ å¼€å§‹è¿½è¸ªè§†é¢‘: {args.input}")
        results = demo.track_video(
            args.input, 
            initial_bbox, 
            args.output, 
            save_results=not args.no_save
        )
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        demo.print_statistics(results)
        
        print(f"\nâœ… è¿½è¸ªå®Œæˆ!")
        if args.output:
            print(f"ğŸ“¹ è¾“å‡ºè§†é¢‘: {args.output}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ è¿½è¸ªå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())
