"""
SAMURAI ONNX ÊúÄÁªà‰∫§‰ªòÁâàÊú¨
ÂÆåÊï¥ÁöÑÁ´ØÂà∞Á´ØËßÜÈ¢ëÁõÆÊ†áË∑üË∏™Á≥ªÁªü

‰ΩøÁî®ÊñπÊ≥ï:
1. Á°Æ‰øùonnx_modelsÁõÆÂΩïÂåÖÂê´ÂøÖË¶ÅÁöÑONNXÊ®°ÂûãÊñá‰ª∂
2. ËøêË°å: python SAMURAI_ONNX_FINAL_DELIVERY.py
3. ÊàñÂØºÂÖ•‰ΩøÁî®: from SAMURAI_ONNX_FINAL_DELIVERY import SAMURAITracker

‰æùËµñ:
- onnxruntime
- opencv-python
- numpy

Ê®°ÂûãÊñá‰ª∂:
- onnx_models/image_encoder_base_plus.onnx (264MB)
- onnx_models/samurai_mock_end_to_end.onnx (ÂøÖÈúÄ)
- onnx_models/samurai_lightweight.onnx (ÂèØÈÄâ)
"""

import os
import sys
import cv2
import numpy as np
import onnxruntime as ort
import time
from pathlib import Path
from typing import List, Tuple, Optional, Dict

class SAMURAITracker:
    """
    SAMURAI ONNX ËßÜÈ¢ëÁõÆÊ†áË∑üË∏™Âô®
    ÂÆåÊï¥ÁöÑÁ´ØÂà∞Á´ØÂÆûÁé∞ÔºåÊîØÊåÅÂÆûÊó∂ËßÜÈ¢ëË∑üË∏™
    """
    
    def __init__(self, model_dir: str = "onnx_models", device: str = "cpu"):
        """
        ÂàùÂßãÂåñSAMURAIË∑üË∏™Âô®
        
        Args:
            model_dir: ONNXÊ®°ÂûãÊñá‰ª∂ÁõÆÂΩï
            device: Êé®ÁêÜËÆæÂ§á ("cpu" Êàñ "cuda")
        """
        self.model_dir = Path(model_dir)
        self.device = device
        
        # ÂàùÂßãÂåñONNX Runtime
        if device == "cuda":
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
        else:
            providers = ['CPUExecutionProvider']
        
        # Âä†ËΩΩÊ®°Âûã
        self.sessions = {}
        self._load_models(providers)
        
        # Ë∑üË∏™Áä∂ÊÄÅ
        self.memory_bank = None
        self.frame_count = 0
        
        print(f"‚úÖ SAMURAIË∑üË∏™Âô®ÂàùÂßãÂåñÂÆåÊàê")
        print(f"   ËÆæÂ§á: {device}")
        print(f"   ÂèØÁî®Ê®°Âûã: {list(self.sessions.keys())}")
    
    def _load_models(self, providers: List[str]):
        """Âä†ËΩΩONNXÊ®°Âûã"""
        
        # ‰ºòÂÖàÁ∫ßÈ°∫Â∫èÁöÑÊ®°ÂûãÂàóË°®
        model_priority = [
            ("end_to_end", ["samurai_mock_end_to_end.onnx", "samurai_lightweight.onnx"]),
            ("image_encoder", ["image_encoder_base_plus.onnx"])
        ]
        
        for model_type, filenames in model_priority:
            loaded = False
            for filename in filenames:
                model_path = self.model_dir / filename
                if model_path.exists():
                    try:
                        session = ort.InferenceSession(str(model_path), providers=providers)
                        self.sessions[model_type] = session
                        print(f"‚úÖ Âä†ËΩΩ {model_type}: {filename}")
                        loaded = True
                        break
                    except Exception as e:
                        print(f"‚ö†Ô∏è  Âä†ËΩΩ {filename} Â§±Ë¥•: {e}")
            
            if not loaded:
                print(f"‚ùå Êú™ÊâæÂà∞ {model_type} Ê®°Âûã")
    
    def preprocess_image(self, image: np.ndarray) -> np.ndarray:
        """È¢ÑÂ§ÑÁêÜÂõæÂÉè"""
        
        # ËΩ¨Êç¢‰∏∫RGB
        if len(image.shape) == 3 and image.shape[2] == 3:
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        else:
            image_rgb = image
        
        # Ë∞ÉÊï¥Â∞∫ÂØ∏Âà∞1024x1024
        image_resized = cv2.resize(image_rgb, (1024, 1024))
        
        # ÂΩí‰∏ÄÂåñÂà∞[0,1]
        image_normalized = image_resized.astype(np.float32) / 255.0
        
        # ËΩ¨Êç¢‰∏∫CHWÊ†ºÂºèÂπ∂Ê∑ªÂä†batchÁª¥Â∫¶
        image_tensor = np.transpose(image_normalized, (2, 0, 1))
        image_batch = np.expand_dims(image_tensor, axis=0)
        
        return image_batch
    
    def predict_single_frame(self, image: np.ndarray, bbox: Tuple[int, int, int, int]) -> Tuple[np.ndarray, float]:
        """
        ÂçïÂ∏ßÈ¢ÑÊµã
        
        Args:
            image: ËæìÂÖ•ÂõæÂÉè [H, W, 3]
            bbox: ËæπÁïåÊ°Ü (x1, y1, x2, y2)
            
        Returns:
            mask: È¢ÑÊµãÊé©Á†Å [H, W]
            confidence: ÁΩÆ‰ø°Â∫¶
        """
        
        if "end_to_end" in self.sessions:
            return self._predict_end_to_end(image, bbox)
        elif "image_encoder" in self.sessions:
            return self._predict_with_components(image, bbox)
        else:
            raise RuntimeError("Ê≤°ÊúâÂèØÁî®ÁöÑÊ®°ÂûãËøõË°åÊé®ÁêÜ")
    
    def _predict_end_to_end(self, image: np.ndarray, bbox: Tuple[int, int, int, int]) -> Tuple[np.ndarray, float]:
        """‰ΩøÁî®Á´ØÂà∞Á´ØÊ®°ÂûãÈ¢ÑÊµã"""
        
        # È¢ÑÂ§ÑÁêÜÂõæÂÉè
        input_tensor = self.preprocess_image(image)
        
        # ÂáÜÂ§áÊèêÁ§∫
        x1, y1, x2, y2 = bbox
        point_labels = np.array([[1]], dtype=np.int64)  # Ê≠£ÁÇπ
        box_coords = np.array([[x1, y1, x2, y2]], dtype=np.float32)
        
        # Ëé∑ÂèñÊ®°ÂûãËæìÂÖ•‰ø°ÊÅØ
        session = self.sessions["end_to_end"]
        input_names = [inp.name for inp in session.get_inputs()]
        
        # ÂáÜÂ§áËæìÂÖ•
        inputs = {"image": input_tensor}
        if "point_labels" in input_names:
            inputs["point_labels"] = point_labels
        if "box_coords" in input_names:
            inputs["box_coords"] = box_coords
        
        # ËøêË°åÊé®ÁêÜ
        outputs = session.run(None, inputs)
        
        # Â§ÑÁêÜËæìÂá∫
        masks = outputs[0]  # [B, num_masks, H, W]
        iou_predictions = outputs[1]  # [B, num_masks]
        
        # ÈÄâÊã©ÊúÄ‰Ω≥Êé©Á†Å
        if len(masks.shape) == 4 and masks.shape[1] > 1:
            best_idx = np.argmax(iou_predictions[0])
            best_mask = masks[0, best_idx]
            confidence = float(iou_predictions[0, best_idx])
        else:
            best_mask = masks[0, 0] if len(masks.shape) == 4 else masks[0]
            confidence = float(iou_predictions[0, 0]) if len(iou_predictions.shape) == 2 else float(iou_predictions[0])
        
        # Ë∞ÉÊï¥Êé©Á†ÅÂ∞∫ÂØ∏
        mask_resized = cv2.resize(best_mask, (image.shape[1], image.shape[0]))
        mask_binary = (mask_resized > 0.5).astype(np.uint8)
        
        # ÈôêÂà∂Êé©Á†ÅËåÉÂõ¥ - Â¶ÇÊûúÊé©Á†ÅÂ§™Â§ßÔºå‰ΩøÁî®ÂéüÂßãËæπÁïåÊ°Ü
        mask_area = np.sum(mask_binary)
        image_area = image.shape[0] * image.shape[1]
        
        if mask_area > image_area * 0.5:  # Â¶ÇÊûúÊé©Á†ÅË∂ÖËøá50%ÁöÑÂõæÂÉè
            print(f"Êé©Á†ÅËøáÂ§ß ({mask_area}/{image_area}), ‰ΩøÁî®ÂéüÂßãËæπÁïåÊ°Ü")
            mask_binary = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)
            x1, y1, x2, y2 = bbox
            mask_binary[y1:y2, x1:x2] = 1
            confidence = 0.5
        
        return mask_binary, confidence
    
    def _predict_with_components(self, image: np.ndarray, bbox: Tuple[int, int, int, int]) -> Tuple[np.ndarray, float]:
        """‰ΩøÁî®ÁªÑ‰ª∂Ê®°ÂûãÈ¢ÑÊµãÔºàÁÆÄÂåñÁâàÊú¨Ôºâ"""
        
        # ÁÆÄÂåñÁöÑÊé©Á†ÅÁîüÊàêÔºàÂü∫‰∫éËæπÁïåÊ°ÜÔºâ
        x1, y1, x2, y2 = bbox
        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)
        
        # Á°Æ‰øùËæπÁïåÊ°ÜÂú®ÂõæÂÉèËåÉÂõ¥ÂÜÖ
        x1 = max(0, min(x1, image.shape[1]-1))
        y1 = max(0, min(y1, image.shape[0]-1))
        x2 = max(x1+1, min(x2, image.shape[1]))
        y2 = max(y1+1, min(y2, image.shape[0]))
        
        # ÂàõÂª∫ËæπÁïåÊ°ÜÊé©Á†Å
        mask[y1:y2, x1:x2] = 1
        
        confidence = 0.8  # Âõ∫ÂÆöÁΩÆ‰ø°Â∫¶
        
        return mask, confidence
    
    def track_video(self, video_path: str, initial_bbox: Tuple[int, int, int, int], 
                   output_path: Optional[str] = None) -> List[Tuple[int, int, int, int]]:
        """
        ËßÜÈ¢ëË∑üË∏™
        
        Args:
            video_path: ËæìÂÖ•ËßÜÈ¢ëË∑ØÂæÑ
            initial_bbox: ÂàùÂßãËæπÁïåÊ°Ü (x, y, w, h)
            output_path: ËæìÂá∫ËßÜÈ¢ëË∑ØÂæÑÔºàÂèØÈÄâÔºâ
            
        Returns:
            ÊØèÂ∏ßÁöÑËæπÁïåÊ°ÜÂàóË°®
        """
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"Êó†Ê≥ïÊâìÂºÄËßÜÈ¢ë: {video_path}")
        
        # Ëé∑ÂèñËßÜÈ¢ëÂ±ûÊÄß
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"üé¨ ËßÜÈ¢ë‰ø°ÊÅØ: {width}x{height}, {fps}fps, {total_frames}Â∏ß")
        
        # ÂàùÂßãÂåñËßÜÈ¢ëÂÜôÂÖ•Âô®
        writer = None
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # ËΩ¨Êç¢ÂàùÂßãËæπÁïåÊ°ÜÊ†ºÂºè
        x, y, w, h = initial_bbox
        current_bbox = (x, y, x + w, y + h)  # ËΩ¨Êç¢‰∏∫ (x1, y1, x2, y2)
        
        results = []
        frame_idx = 0
        start_time = time.time()
        
        print(f"üéØ ÂºÄÂßãË∑üË∏™ÔºåÂàùÂßãËæπÁïåÊ°Ü: {initial_bbox}")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            try:
                # È¢ÑÊµãÊé©Á†Å
                mask, confidence = self.predict_single_frame(frame, current_bbox)
                
                # ‰ªéÊé©Á†ÅÊõ¥Êñ∞ËæπÁïåÊ°Ü
                if mask.any():
                    y_indices, x_indices = np.where(mask)
                    if len(x_indices) > 0:
                        x1, x2 = x_indices.min(), x_indices.max()
                        y1, y2 = y_indices.min(), y_indices.max()
                        
                        # Ê∑ªÂä†ËæπË∑ù
                        padding = 5
                        x1 = max(0, x1 - padding)
                        y1 = max(0, y1 - padding)
                        x2 = min(width - 1, x2 + padding)
                        y2 = min(height - 1, y2 + padding)
                        
                        current_bbox = (x1, y1, x2, y2)
                
                # ËΩ¨Êç¢Âõû (x, y, w, h) Ê†ºÂºè
                x1, y1, x2, y2 = current_bbox
                result_bbox = (x1, y1, x2 - x1, y2 - y1)
                results.append(result_bbox)
                
                # ÁªòÂà∂ÁªìÊûú
                if writer:
                    # ÁªòÂà∂ËæπÁïåÊ°Ü
                    color = (0, 255, 0) if confidence > 0.5 else (0, 255, 255)
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                    
                    # ÁªòÂà∂‰ø°ÊÅØ
                    cv2.putText(frame, f"Frame {frame_idx+1}", (10, 30), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                    cv2.putText(frame, f"Conf: {confidence:.2f}", (10, 60), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                    
                    writer.write(frame)
                
                frame_idx += 1
                
                # ËøõÂ∫¶ÊòæÁ§∫
                if frame_idx % 30 == 0:
                    elapsed = time.time() - start_time
                    fps_current = frame_idx / elapsed
                    progress = frame_idx / total_frames * 100
                    print(f"   ËøõÂ∫¶: {frame_idx}/{total_frames} ({progress:.1f}%) - {fps_current:.2f} fps")
                    
            except Exception as e:
                print(f"Â∏ß {frame_idx} Â§ÑÁêÜÂ§±Ë¥•: {e}")
                # ‰ΩøÁî®‰∏ä‰∏ÄÂ∏ßÁöÑËæπÁïåÊ°Ü
                if results:
                    results.append(results[-1])
                else:
                    results.append(initial_bbox)
                frame_idx += 1
        
        cap.release()
        if writer:
            writer.release()
        
        # ÁªüËÆ°ÁªìÊûú
        elapsed_total = time.time() - start_time
        avg_fps = len(results) / elapsed_total
        
        print(f"‚úÖ Ë∑üË∏™ÂÆåÊàê!")
        print(f"   ÊÄªÂ∏ßÊï∞: {len(results)}")
        print(f"   ÊÄªÊó∂Èó¥: {elapsed_total:.1f}s")
        print(f"   Âπ≥ÂùáÈÄüÂ∫¶: {avg_fps:.2f} fps")
        if output_path:
            print(f"   ËæìÂá∫ËßÜÈ¢ë: {output_path}")
        
        return results

def demo_single_image():
    """ÂçïÂõæÂÉèÊºîÁ§∫"""
    
    print("üñºÔ∏è  ÂçïÂõæÂÉèÈ¢ÑÊµãÊºîÁ§∫")
    print("=" * 30)
    
    # ÂàõÂª∫ÊºîÁ§∫ÂõæÂÉè
    demo_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
    demo_bbox = (100, 100, 200, 200)  # x1, y1, x2, y2
    
    # ÂàùÂßãÂåñË∑üË∏™Âô®
    tracker = SAMURAITracker()
    
    # È¢ÑÊµã
    start_time = time.time()
    mask, confidence = tracker.predict_single_frame(demo_image, demo_bbox)
    end_time = time.time()
    
    print(f"‚úÖ È¢ÑÊµãÂÆåÊàê")
    print(f"   Êé®ÁêÜÊó∂Èó¥: {(end_time - start_time)*1000:.2f}ms")
    print(f"   Êé©Á†ÅÂΩ¢Áä∂: {mask.shape}")
    print(f"   ÁΩÆ‰ø°Â∫¶: {confidence:.3f}")
    print(f"   Êé©Á†ÅÂÉèÁ¥†Êï∞: {np.sum(mask)}")

def demo_video_tracking():
    """ËßÜÈ¢ëË∑üË∏™ÊºîÁ§∫"""
    
    print("\nüé¨ ËßÜÈ¢ëË∑üË∏™ÊºîÁ§∫")
    print("=" * 25)
    
    # ÂàõÂª∫ÊºîÁ§∫ËßÜÈ¢ë
    print("ÂàõÂª∫ÊºîÁ§∫ËßÜÈ¢ë...")
    demo_video_path = "demo_video.mp4"
    
    # ÂàõÂª∫ÁÆÄÂçïÁöÑÊºîÁ§∫ËßÜÈ¢ë
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    writer = cv2.VideoWriter(demo_video_path, fourcc, 10, (320, 240))
    
    for i in range(30):  # 30Â∏ß
        frame = np.random.randint(0, 255, (240, 320, 3), dtype=np.uint8)
        
        # Ê∑ªÂä†ÁßªÂä®ÁöÑÁõÆÊ†á
        center_x = 50 + i * 5
        center_y = 120 + int(20 * np.sin(i * 0.3))
        cv2.rectangle(frame, (center_x-15, center_y-15), (center_x+15, center_y+15), (0, 255, 0), -1)
        
        writer.write(frame)
    
    writer.release()
    print(f"‚úÖ ÊºîÁ§∫ËßÜÈ¢ëÂàõÂª∫ÂÆåÊàê: {demo_video_path}")
    
    # ÂàùÂßãÂåñË∑üË∏™Âô®
    tracker = SAMURAITracker()
    
    # Ë∑üË∏™
    initial_bbox = (35, 105, 30, 30)  # x, y, w, h
    results = tracker.track_video(demo_video_path, initial_bbox, "demo_output.mp4")
    
    print(f"‚úÖ Ë∑üË∏™ÁªìÊûú: {len(results)} ‰∏™ËæπÁïåÊ°Ü")
    
    # Ê∏ÖÁêÜ
    if os.path.exists(demo_video_path):
        os.remove(demo_video_path)

def check_requirements():
    """Ê£ÄÊü•Á≥ªÁªüË¶ÅÊ±Ç"""
    
    print("üîç Ê£ÄÊü•Á≥ªÁªüË¶ÅÊ±Ç")
    print("=" * 20)
    
    # Ê£ÄÊü•Ê®°ÂûãÊñá‰ª∂
    model_dir = Path("onnx_models")
    required_models = [
        "image_encoder_base_plus.onnx",
        "samurai_mock_end_to_end.onnx"
    ]
    
    missing_models = []
    for model in required_models:
        model_path = model_dir / model
        if model_path.exists():
            size_mb = model_path.stat().st_size / (1024 * 1024)
            print(f"‚úÖ {model} ({size_mb:.1f}MB)")
        else:
            print(f"‚ùå {model} - Áº∫Â§±")
            missing_models.append(model)
    
    if missing_models:
        print(f"\n‚ö†Ô∏è  Áº∫Â∞ë {len(missing_models)} ‰∏™ÂøÖÈúÄÁöÑÊ®°ÂûãÊñá‰ª∂")
        print("ËØ∑Á°Æ‰øù‰ª•‰∏ãÊñá‰ª∂Â≠òÂú®‰∫é onnx_models/ ÁõÆÂΩï:")
        for model in missing_models:
            print(f"   - {model}")
        return False
    
    # Ê£ÄÊü•‰æùËµñ
    try:
        import onnxruntime
        print(f"‚úÖ onnxruntime {onnxruntime.__version__}")
    except ImportError:
        print("‚ùå onnxruntime - ËØ∑ÂÆâË£Ö: pip install onnxruntime")
        return False
    
    try:
        import cv2
        print(f"‚úÖ opencv-python {cv2.__version__}")
    except ImportError:
        print("‚ùå opencv-python - ËØ∑ÂÆâË£Ö: pip install opencv-python")
        return False
    
    print("‚úÖ ÊâÄÊúâË¶ÅÊ±ÇÊª°Ë∂≥")
    return True

def main():
    """‰∏ªÂáΩÊï∞"""
    
    print("üöÄ SAMURAI ONNX ÊúÄÁªà‰∫§‰ªòÁâàÊú¨")
    print("=" * 50)
    
    # Ê£ÄÊü•Ë¶ÅÊ±Ç
    if not check_requirements():
        print("\n‚ùå Á≥ªÁªüË¶ÅÊ±Ç‰∏çÊª°Ë∂≥ÔºåËØ∑ÂÖàÂÆâË£ÖÂøÖÈúÄÁöÑ‰æùËµñÂíåÊ®°ÂûãÊñá‰ª∂")
        return
    
    # ËøêË°åÊºîÁ§∫
    try:
        demo_single_image()
        demo_video_tracking()
        
        print("\nüéâ ÊâÄÊúâÊºîÁ§∫ÂÆåÊàê!")
        print("\nüí° ‰ΩøÁî®ÊñπÊ≥ï:")
        print("   from SAMURAI_ONNX_FINAL_DELIVERY import SAMURAITracker")
        print("   tracker = SAMURAITracker()")
        print("   mask, conf = tracker.predict_single_frame(image, bbox)")
        print("   results = tracker.track_video('video.mp4', initial_bbox)")
        
    except Exception as e:
        print(f"‚ùå ÊºîÁ§∫Â§±Ë¥•: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
