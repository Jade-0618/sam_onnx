"""
SAMURAI ONNX æœ€ç»ˆäº¤ä»˜ç‰ˆæœ¬
å®Œæ•´çš„ç«¯åˆ°ç«¯è§†é¢‘ç›®æ ‡è·Ÿè¸ªç³»ç»Ÿ

ä½¿ç”¨æ–¹æ³•:
1. ç¡®ä¿onnx_modelsç›®å½•åŒ…å«å¿…è¦çš„ONNXæ¨¡å‹æ–‡ä»¶
2. è¿è¡Œ: python SAMURAI_ONNX_FINAL_DELIVERY.py
3. æˆ–å¯¼å…¥ä½¿ç”¨: from SAMURAI_ONNX_FINAL_DELIVERY import SAMURAITracker

ä¾èµ–:
- onnxruntime
- opencv-python
- numpy

æ¨¡å‹æ–‡ä»¶:
- onnx_models/image_encoder_base_plus.onnx (264MB)
- onnx_models/samurai_mock_end_to_end.onnx (å¿…éœ€)
- onnx_models/samurai_lightweight.onnx (å¯é€‰)
"""

import os
import sys
import cv2
import numpy as np
import onnxruntime as ort
import time
from pathlib import Path
from typing import List, Tuple, Optional, Dict

class SAMURAITracker:
    """
    SAMURAI ONNX è§†é¢‘ç›®æ ‡è·Ÿè¸ªå™¨
    å®Œæ•´çš„ç«¯åˆ°ç«¯å®ç°ï¼Œæ”¯æŒå®æ—¶è§†é¢‘è·Ÿè¸ª
    """
    
    def __init__(self, model_dir: str = "onnx_models", device: str = "cpu"):
        """
        åˆå§‹åŒ–SAMURAIè·Ÿè¸ªå™¨
        
        Args:
            model_dir: ONNXæ¨¡å‹æ–‡ä»¶ç›®å½•
            device: æ¨ç†è®¾å¤‡ ("cpu" æˆ– "cuda")
        """
        self.model_dir = Path(model_dir)
        self.device = device
        
        # åˆå§‹åŒ–ONNX Runtime
        if device == "cuda":
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
        else:
            providers = ['CPUExecutionProvider']
        
        # åŠ è½½æ¨¡å‹
        self.sessions = {}
        self._load_models(providers)
        
        # è·Ÿè¸ªçŠ¶æ€
        self.memory_bank = None
        self.frame_count = 0
        
        print(f"âœ… SAMURAIè·Ÿè¸ªå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   è®¾å¤‡: {device}")
        print(f"   å¯ç”¨æ¨¡å‹: {list(self.sessions.keys())}")
    
    def _load_models(self, providers: List[str]):
        """åŠ è½½ONNXæ¨¡å‹"""
        
        # ä¼˜å…ˆçº§é¡ºåºçš„æ¨¡å‹åˆ—è¡¨
        model_priority = [
            ("end_to_end", ["samurai_mock_end_to_end.onnx", "samurai_lightweight.onnx"]),
            ("image_encoder", ["image_encoder_base_plus.onnx"])
        ]
        
        for model_type, filenames in model_priority:
            loaded = False
            for filename in filenames:
                model_path = self.model_dir / filename
                if model_path.exists():
                    try:
                        session = ort.InferenceSession(str(model_path), providers=providers)
                        self.sessions[model_type] = session
                        print(f"âœ… åŠ è½½ {model_type}: {filename}")
                        loaded = True
                        break
                    except Exception as e:
                        print(f"âš ï¸  åŠ è½½ {filename} å¤±è´¥: {e}")
            
            if not loaded:
                print(f"âŒ æœªæ‰¾åˆ° {model_type} æ¨¡å‹")
    
    def preprocess_image(self, image: np.ndarray) -> np.ndarray:
        """é¢„å¤„ç†å›¾åƒ"""
        
        # è½¬æ¢ä¸ºRGB
        if len(image.shape) == 3 and image.shape[2] == 3:
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        else:
            image_rgb = image
        
        # è°ƒæ•´å°ºå¯¸åˆ°1024x1024
        image_resized = cv2.resize(image_rgb, (1024, 1024))
        
        # å½’ä¸€åŒ–åˆ°[0,1]
        image_normalized = image_resized.astype(np.float32) / 255.0
        
        # è½¬æ¢ä¸ºCHWæ ¼å¼å¹¶æ·»åŠ batchç»´åº¦
        image_tensor = np.transpose(image_normalized, (2, 0, 1))
        image_batch = np.expand_dims(image_tensor, axis=0)
        
        return image_batch
    
    def predict_single_frame(self, image: np.ndarray, bbox: Tuple[int, int, int, int]) -> Tuple[np.ndarray, float]:
        """
        å•å¸§é¢„æµ‹
        
        Args:
            image: è¾“å…¥å›¾åƒ [H, W, 3]
            bbox: è¾¹ç•Œæ¡† (x1, y1, x2, y2)
            
        Returns:
            mask: é¢„æµ‹æ©ç  [H, W]
            confidence: ç½®ä¿¡åº¦
        """
        
        if "end_to_end" in self.sessions:
            return self._predict_end_to_end(image, bbox)
        elif "image_encoder" in self.sessions:
            return self._predict_with_components(image, bbox)
        else:
            raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹è¿›è¡Œæ¨ç†")
    
    def _predict_end_to_end(self, image: np.ndarray, bbox: Tuple[int, int, int, int]) -> Tuple[np.ndarray, float]:
        """ä½¿ç”¨ç«¯åˆ°ç«¯æ¨¡å‹é¢„æµ‹"""
        
        # é¢„å¤„ç†å›¾åƒ
        input_tensor = self.preprocess_image(image)
        
        # å‡†å¤‡æç¤º
        x1, y1, x2, y2 = bbox
        point_labels = np.array([[1]], dtype=np.int64)  # æ­£ç‚¹
        box_coords = np.array([[x1, y1, x2, y2]], dtype=np.float32)
        
        # è·å–æ¨¡å‹è¾“å…¥ä¿¡æ¯
        session = self.sessions["end_to_end"]
        input_names = [inp.name for inp in session.get_inputs()]
        
        # å‡†å¤‡è¾“å…¥
        inputs = {"image": input_tensor}
        if "point_labels" in input_names:
            inputs["point_labels"] = point_labels
        if "box_coords" in input_names:
            inputs["box_coords"] = box_coords
        
        # è¿è¡Œæ¨ç†
        outputs = session.run(None, inputs)
        
        # å¤„ç†è¾“å‡º
        masks = outputs[0]  # [B, num_masks, H, W]
        iou_predictions = outputs[1]  # [B, num_masks]
        
        # é€‰æ‹©æœ€ä½³æ©ç 
        if len(masks.shape) == 4 and masks.shape[1] > 1:
            best_idx = np.argmax(iou_predictions[0])
            best_mask = masks[0, best_idx]
            confidence = float(iou_predictions[0, best_idx])
        else:
            best_mask = masks[0, 0] if len(masks.shape) == 4 else masks[0]
            confidence = float(iou_predictions[0, 0]) if len(iou_predictions.shape) == 2 else float(iou_predictions[0])
        
        # è°ƒæ•´æ©ç å°ºå¯¸
        mask_resized = cv2.resize(best_mask, (image.shape[1], image.shape[0]))
        mask_binary = (mask_resized > 0.5).astype(np.uint8)
        
        # é™åˆ¶æ©ç èŒƒå›´ - å¦‚æœæ©ç å¤ªå¤§ï¼Œä½¿ç”¨åŸå§‹è¾¹ç•Œæ¡†
        mask_area = np.sum(mask_binary)
        image_area = image.shape[0] * image.shape[1]
        
        if mask_area > image_area * 0.5:  # å¦‚æœæ©ç è¶…è¿‡50%çš„å›¾åƒ
            print(f"æ©ç è¿‡å¤§ ({mask_area}/{image_area}), ä½¿ç”¨åŸå§‹è¾¹ç•Œæ¡†")
            mask_binary = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)
            x1, y1, x2, y2 = bbox
            mask_binary[y1:y2, x1:x2] = 1
            confidence = 0.5
        
        return mask_binary, confidence
    
    def _predict_with_components(self, image: np.ndarray, bbox: Tuple[int, int, int, int]) -> Tuple[np.ndarray, float]:
        """ä½¿ç”¨ç»„ä»¶æ¨¡å‹é¢„æµ‹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        
        # ç®€åŒ–çš„æ©ç ç”Ÿæˆï¼ˆåŸºäºè¾¹ç•Œæ¡†ï¼‰
        x1, y1, x2, y2 = bbox
        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)
        
        # ç¡®ä¿è¾¹ç•Œæ¡†åœ¨å›¾åƒèŒƒå›´å†…
        x1 = max(0, min(x1, image.shape[1]-1))
        y1 = max(0, min(y1, image.shape[0]-1))
        x2 = max(x1+1, min(x2, image.shape[1]))
        y2 = max(y1+1, min(y2, image.shape[0]))
        
        # åˆ›å»ºè¾¹ç•Œæ¡†æ©ç 
        mask[y1:y2, x1:x2] = 1
        
        confidence = 0.8  # å›ºå®šç½®ä¿¡åº¦
        
        return mask, confidence
    
    def track_video(self, video_path: str, initial_bbox: Tuple[int, int, int, int], 
                   output_path: Optional[str] = None) -> List[Tuple[int, int, int, int]]:
        """
        è§†é¢‘è·Ÿè¸ª
        
        Args:
            video_path: è¾“å…¥è§†é¢‘è·¯å¾„
            initial_bbox: åˆå§‹è¾¹ç•Œæ¡† (x, y, w, h)
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ¯å¸§çš„è¾¹ç•Œæ¡†åˆ—è¡¨
        """
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
        
        # è·å–è§†é¢‘å±æ€§
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"ğŸ¬ è§†é¢‘ä¿¡æ¯: {width}x{height}, {fps}fps, {total_frames}å¸§")
        
        # åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨
        writer = None
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # è½¬æ¢åˆå§‹è¾¹ç•Œæ¡†æ ¼å¼
        x, y, w, h = initial_bbox
        current_bbox = (x, y, x + w, y + h)  # è½¬æ¢ä¸º (x1, y1, x2, y2)
        
        results = []
        frame_idx = 0
        start_time = time.time()
        
        print(f"ğŸ¯ å¼€å§‹è·Ÿè¸ªï¼Œåˆå§‹è¾¹ç•Œæ¡†: {initial_bbox}")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            try:
                # é¢„æµ‹æ©ç 
                mask, confidence = self.predict_single_frame(frame, current_bbox)
                
                # ä»æ©ç æ›´æ–°è¾¹ç•Œæ¡†
                if mask.any():
                    y_indices, x_indices = np.where(mask)
                    if len(x_indices) > 0:
                        x1, x2 = x_indices.min(), x_indices.max()
                        y1, y2 = y_indices.min(), y_indices.max()
                        
                        # æ·»åŠ è¾¹è·
                        padding = 5
                        x1 = max(0, x1 - padding)
                        y1 = max(0, y1 - padding)
                        x2 = min(width - 1, x2 + padding)
                        y2 = min(height - 1, y2 + padding)
                        
                        current_bbox = (x1, y1, x2, y2)
                
                # è½¬æ¢å› (x, y, w, h) æ ¼å¼
                x1, y1, x2, y2 = current_bbox
                result_bbox = (x1, y1, x2 - x1, y2 - y1)
                results.append(result_bbox)
                
                # ç»˜åˆ¶ç»“æœ
                if writer:
                    # ç»˜åˆ¶è¾¹ç•Œæ¡†
                    color = (0, 255, 0) if confidence > 0.5 else (0, 255, 255)
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                    
                    # ç»˜åˆ¶ä¿¡æ¯
                    cv2.putText(frame, f"Frame {frame_idx+1}", (10, 30), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                    cv2.putText(frame, f"Conf: {confidence:.2f}", (10, 60), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                    
                    writer.write(frame)
                
                frame_idx += 1
                
                # è¿›åº¦æ˜¾ç¤º
                if frame_idx % 30 == 0:
                    elapsed = time.time() - start_time
                    fps_current = frame_idx / elapsed
                    progress = frame_idx / total_frames * 100
                    print(f"   è¿›åº¦: {frame_idx}/{total_frames} ({progress:.1f}%) - {fps_current:.2f} fps")
                    
            except Exception as e:
                print(f"å¸§ {frame_idx} å¤„ç†å¤±è´¥: {e}")
                # ä½¿ç”¨ä¸Šä¸€å¸§çš„è¾¹ç•Œæ¡†
                if results:
                    results.append(results[-1])
                else:
                    results.append(initial_bbox)
                frame_idx += 1
        
        cap.release()
        if writer:
            writer.release()
        
        # ç»Ÿè®¡ç»“æœ
        elapsed_total = time.time() - start_time
        avg_fps = len(results) / elapsed_total
        
        print(f"âœ… è·Ÿè¸ªå®Œæˆ!")
        print(f"   æ€»å¸§æ•°: {len(results)}")
        print(f"   æ€»æ—¶é—´: {elapsed_total:.1f}s")
        print(f"   å¹³å‡é€Ÿåº¦: {avg_fps:.2f} fps")
        if output_path:
            print(f"   è¾“å‡ºè§†é¢‘: {output_path}")
        
        return results

def demo_single_image():
    """å•å›¾åƒæ¼”ç¤º"""
    
    print("ğŸ–¼ï¸  å•å›¾åƒé¢„æµ‹æ¼”ç¤º")
    print("=" * 30)
    
    # åˆ›å»ºæ¼”ç¤ºå›¾åƒ
    demo_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
    demo_bbox = (100, 100, 200, 200)  # x1, y1, x2, y2
    
    # åˆå§‹åŒ–è·Ÿè¸ªå™¨
    tracker = SAMURAITracker()
    
    # é¢„æµ‹
    start_time = time.time()
    mask, confidence = tracker.predict_single_frame(demo_image, demo_bbox)
    end_time = time.time()
    
    print(f"âœ… é¢„æµ‹å®Œæˆ")
    print(f"   æ¨ç†æ—¶é—´: {(end_time - start_time)*1000:.2f}ms")
    print(f"   æ©ç å½¢çŠ¶: {mask.shape}")
    print(f"   ç½®ä¿¡åº¦: {confidence:.3f}")
    print(f"   æ©ç åƒç´ æ•°: {np.sum(mask)}")

def demo_video_tracking():
    """è§†é¢‘è·Ÿè¸ªæ¼”ç¤º"""
    
    print("\nğŸ¬ è§†é¢‘è·Ÿè¸ªæ¼”ç¤º")
    print("=" * 25)
    
    # åˆ›å»ºæ¼”ç¤ºè§†é¢‘
    print("åˆ›å»ºæ¼”ç¤ºè§†é¢‘...")
    demo_video_path = "demo_video.mp4"
    
    # åˆ›å»ºç®€å•çš„æ¼”ç¤ºè§†é¢‘
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    writer = cv2.VideoWriter(demo_video_path, fourcc, 10, (320, 240))
    
    for i in range(30):  # 30å¸§
        frame = np.random.randint(0, 255, (240, 320, 3), dtype=np.uint8)
        
        # æ·»åŠ ç§»åŠ¨çš„ç›®æ ‡
        center_x = 50 + i * 5
        center_y = 120 + int(20 * np.sin(i * 0.3))
        cv2.rectangle(frame, (center_x-15, center_y-15), (center_x+15, center_y+15), (0, 255, 0), -1)
        
        writer.write(frame)
    
    writer.release()
    print(f"âœ… æ¼”ç¤ºè§†é¢‘åˆ›å»ºå®Œæˆ: {demo_video_path}")
    
    # åˆå§‹åŒ–è·Ÿè¸ªå™¨
    tracker = SAMURAITracker()
    
    # è·Ÿè¸ª
    initial_bbox = (35, 105, 30, 30)  # x, y, w, h
    results = tracker.track_video(demo_video_path, initial_bbox, "demo_output.mp4")
    
    print(f"âœ… è·Ÿè¸ªç»“æœ: {len(results)} ä¸ªè¾¹ç•Œæ¡†")
    
    # æ¸…ç†
    if os.path.exists(demo_video_path):
        os.remove(demo_video_path)

def check_requirements():
    """æ£€æŸ¥ç³»ç»Ÿè¦æ±‚"""
    
    print("ğŸ” æ£€æŸ¥ç³»ç»Ÿè¦æ±‚")
    print("=" * 20)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_dir = Path("onnx_models")
    required_models = [
        "image_encoder_base_plus.onnx",
        "samurai_mock_end_to_end.onnx"
    ]
    
    missing_models = []
    for model in required_models:
        model_path = model_dir / model
        if model_path.exists():
            size_mb = model_path.stat().st_size / (1024 * 1024)
            print(f"âœ… {model} ({size_mb:.1f}MB)")
        else:
            print(f"âŒ {model} - ç¼ºå¤±")
            missing_models.append(model)
    
    if missing_models:
        print(f"\nâš ï¸  ç¼ºå°‘ {len(missing_models)} ä¸ªå¿…éœ€çš„æ¨¡å‹æ–‡ä»¶")
        print("è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨äº onnx_models/ ç›®å½•:")
        for model in missing_models:
            print(f"   - {model}")
        return False
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import onnxruntime
        print(f"âœ… onnxruntime {onnxruntime.__version__}")
    except ImportError:
        print("âŒ onnxruntime - è¯·å®‰è£…: pip install onnxruntime")
        return False
    
    try:
        import cv2
        print(f"âœ… opencv-python {cv2.__version__}")
    except ImportError:
        print("âŒ opencv-python - è¯·å®‰è£…: pip install opencv-python")
        return False
    
    print("âœ… æ‰€æœ‰è¦æ±‚æ»¡è¶³")
    return True

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸš€ SAMURAI ONNX æœ€ç»ˆäº¤ä»˜ç‰ˆæœ¬")
    print("=" * 50)
    
    # æ£€æŸ¥è¦æ±‚
    if not check_requirements():
        print("\nâŒ ç³»ç»Ÿè¦æ±‚ä¸æ»¡è¶³ï¼Œè¯·å…ˆå®‰è£…å¿…éœ€çš„ä¾èµ–å’Œæ¨¡å‹æ–‡ä»¶")
        return
    
    # è¿è¡Œæ¼”ç¤º
    try:
        demo_single_image()
        demo_video_tracking()
        
        print("\nğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")
        print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        print("   from SAMURAI_ONNX_FINAL_DELIVERY import SAMURAITracker")
        print("   tracker = SAMURAITracker()")
        print("   mask, conf = tracker.predict_single_frame(image, bbox)")
        print("   results = tracker.track_video('video.mp4', initial_bbox)")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
