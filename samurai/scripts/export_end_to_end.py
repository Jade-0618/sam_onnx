"""
ç«¯åˆ°ç«¯SAMURAI ONNXæ¨¡å‹å¯¼å‡º
å°†æ‰€æœ‰ç»„ä»¶æ•´åˆä¸ºå•ä¸€çš„å®Œæ•´ONNXæ¨¡å‹
"""

import os
import sys
import torch
import torch.nn as nn
import numpy as np
from typing import List, Tuple, Optional, Dict

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sam2_path = os.path.join(project_root, "sam2")
sys.path.insert(0, sam2_path)

from sam2.build_sam import build_sam2_video_predictor

class SAMURAIEndToEndONNX(nn.Module):
    """
    å®Œæ•´çš„ç«¯åˆ°ç«¯SAMURAI ONNXæ¨¡å‹
    æ•´åˆæ‰€æœ‰ç»„ä»¶ï¼šå›¾åƒç¼–ç å™¨ã€æç¤ºç¼–ç å™¨ã€æ©ç è§£ç å™¨ã€å†…å­˜ç¼–ç å™¨
    """
    
    def __init__(self, predictor):
        super().__init__()
        
        # ä¿å­˜æ‰€æœ‰ç»„ä»¶
        self.image_encoder = predictor.image_encoder
        self.prompt_encoder = predictor.sam_prompt_encoder
        self.mask_decoder = predictor.sam_mask_decoder
        self.memory_encoder = predictor.memory_encoder
        
        # æ¨¡å‹å‚æ•°
        self.image_size = predictor.image_size
        self.hidden_dim = getattr(predictor, 'hidden_dim', 256)
        self.num_maskmem = getattr(predictor.memory_encoder, 'num_maskmem', 7)
        
        # å†…å­˜é“¶è¡ŒçŠ¶æ€
        self.register_buffer('memory_bank', torch.zeros(1, self.num_maskmem, 256, 64, 64))
        self.register_buffer('frame_count', torch.tensor(0, dtype=torch.long))
        
    def forward(self, 
                image: torch.Tensor,
                point_coords: torch.Tensor,
                point_labels: torch.Tensor,
                box_coords: Optional[torch.Tensor] = None,
                prev_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å®Œæ•´çš„ç«¯åˆ°ç«¯å‰å‘ä¼ æ’­
        
        Args:
            image: [B, 3, H, W] è¾“å…¥å›¾åƒ
            point_coords: [B, N, 2] ç‚¹åæ ‡
            point_labels: [B, N] ç‚¹æ ‡ç­¾
            box_coords: [B, 4] å¯é€‰çš„è¾¹ç•Œæ¡†åæ ‡
            prev_mask: [B, 1, H, W] å¯é€‰çš„å…ˆå‰æ©ç 
            
        Returns:
            masks: [B, num_masks, H, W] é¢„æµ‹æ©ç 
            iou_predictions: [B, num_masks] IoUé¢„æµ‹
            memory_features: [B, C, H_feat, W_feat] å†…å­˜ç‰¹å¾
        """
        batch_size = image.shape[0]
        device = image.device
        
        # 1. å›¾åƒç¼–ç 
        image_features = self._encode_image(image)
        
        # 2. æç¤ºç¼–ç 
        sparse_embeddings, dense_embeddings = self._encode_prompts(
            point_coords, point_labels, box_coords, prev_mask
        )
        
        # 3. æ©ç è§£ç 
        masks, iou_predictions = self._decode_masks(
            image_features, sparse_embeddings, dense_embeddings
        )
        
        # 4. å†…å­˜ç¼–ç 
        memory_features = self._encode_memory(
            image_features, masks, batch_size, device
        )
        
        return masks, iou_predictions, memory_features
    
    def _encode_image(self, image: torch.Tensor) -> Dict[str, torch.Tensor]:
        """å›¾åƒç¼–ç """
        # ç¡®ä¿å›¾åƒå°ºå¯¸æ­£ç¡®
        if image.shape[-2:] != (self.image_size, self.image_size):
            image = torch.nn.functional.interpolate(
                image, size=(self.image_size, self.image_size),
                mode='bilinear', align_corners=False
            )
        
        # è°ƒç”¨å›¾åƒç¼–ç å™¨
        backbone_out = self.image_encoder(image)
        
        # å¤„ç†ä¸åŒçš„è¾“å‡ºæ ¼å¼
        if isinstance(backbone_out, dict):
            return backbone_out
        elif isinstance(backbone_out, (list, tuple)):
            # å‡è®¾ç¬¬ä¸€ä¸ªæ˜¯ä¸»è¦ç‰¹å¾
            return {'backbone_features': backbone_out[0]}
        else:
            return {'backbone_features': backbone_out}
    
    def _encode_prompts(self, 
                       point_coords: torch.Tensor,
                       point_labels: torch.Tensor,
                       box_coords: Optional[torch.Tensor],
                       prev_mask: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:
        """æç¤ºç¼–ç """
        
        # å‡†å¤‡æç¤º
        points = (point_coords, point_labels) if point_coords is not None else None
        boxes = box_coords
        masks = prev_mask
        
        # è°ƒç”¨æç¤ºç¼–ç å™¨
        sparse_embeddings, dense_embeddings = self.prompt_encoder(
            points=points,
            boxes=boxes,
            masks=masks
        )
        
        return sparse_embeddings, dense_embeddings
    
    def _decode_masks(self,
                     image_features: Dict[str, torch.Tensor],
                     sparse_embeddings: torch.Tensor,
                     dense_embeddings: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """æ©ç è§£ç """
        
        # è·å–ä¸»è¦å›¾åƒç‰¹å¾
        if 'backbone_features' in image_features:
            main_features = image_features['backbone_features']
        else:
            # å–ç¬¬ä¸€ä¸ªç‰¹å¾ä½œä¸ºä¸»è¦ç‰¹å¾
            main_features = list(image_features.values())[0]
        
        # è·å–ä½ç½®ç¼–ç 
        image_pe = self.prompt_encoder.get_dense_pe()
        
        # è°ƒç”¨æ©ç è§£ç å™¨
        masks, iou_predictions, _, _ = self.mask_decoder(
            image_embeddings=main_features,
            image_pe=image_pe,
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=True,
            repeat_image=False,
            high_res_features=None
        )
        
        return masks, iou_predictions
    
    def _encode_memory(self,
                      image_features: Dict[str, torch.Tensor],
                      masks: torch.Tensor,
                      batch_size: int,
                      device: torch.device) -> torch.Tensor:
        """å†…å­˜ç¼–ç """
        
        # è·å–å½“å‰è§†è§‰ç‰¹å¾
        if 'backbone_features' in image_features:
            curr_vision_feats = image_features['backbone_features']
        else:
            curr_vision_feats = list(image_features.values())[0]
        
        # è°ƒæ•´å†…å­˜é“¶è¡Œå¤§å°
        if self.memory_bank.shape[0] != batch_size:
            C, H, W = curr_vision_feats.shape[1], curr_vision_feats.shape[2], curr_vision_feats.shape[3]
            self.memory_bank = torch.zeros(
                batch_size, self.num_maskmem, C, H, W,
                device=device, dtype=curr_vision_feats.dtype
            )
        
        # å‡†å¤‡è¾“å…¥
        feat_sizes = torch.tensor([curr_vision_feats.shape[2], curr_vision_feats.shape[3]], 
                                 dtype=torch.long, device=device)
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ©ç ä½œä¸ºè¾“å‡ºæ©ç 
        output_mask = masks[:, 0:1]  # [B, 1, H, W]
        
        # ç¡®å®šæ˜¯å¦ä¸ºå†…å­˜å¸§
        is_mem_frame = (self.frame_count % 5 == 0).unsqueeze(0).expand(batch_size)
        
        # ç®€åŒ–çš„å†…å­˜ç¼–ç ï¼ˆç”±äºONNXé™åˆ¶ï¼‰
        # è¿™é‡Œæˆ‘ä»¬å®ç°ä¸€ä¸ªç®€åŒ–ä½†åŠŸèƒ½å®Œæ•´çš„ç‰ˆæœ¬
        memory_features = self._simplified_memory_encoding(
            curr_vision_feats, output_mask, is_mem_frame
        )
        
        # æ›´æ–°å¸§è®¡æ•°
        self.frame_count += 1
        
        return memory_features
    
    def _simplified_memory_encoding(self,
                                   curr_vision_feats: torch.Tensor,
                                   output_mask: torch.Tensor,
                                   is_mem_frame: torch.Tensor) -> torch.Tensor:
        """ç®€åŒ–çš„å†…å­˜ç¼–ç å®ç°"""
        
        batch_size = curr_vision_feats.shape[0]
        
        # è°ƒæ•´æ©ç å°ºå¯¸ä»¥åŒ¹é…ç‰¹å¾
        if output_mask.shape[-2:] != curr_vision_feats.shape[-2:]:
            mask_resized = torch.nn.functional.interpolate(
                output_mask,
                size=curr_vision_feats.shape[-2:],
                mode='bilinear',
                align_corners=False
            )
        else:
            mask_resized = output_mask
        
        # åº”ç”¨æ©ç åˆ°ç‰¹å¾
        masked_features = curr_vision_feats * mask_resized
        
        # æ›´æ–°å†…å­˜é“¶è¡Œ
        for b in range(batch_size):
            if is_mem_frame[b]:
                # å¾ªç¯ç§»ä½å†…å­˜
                self.memory_bank[b, 1:] = self.memory_bank[b, :-1].clone()
                self.memory_bank[b, 0] = masked_features[b]
        
        # ç”Ÿæˆå†…å­˜ç‰¹å¾ï¼šå½“å‰ç‰¹å¾ä¸å†…å­˜çš„åŠ æƒç»„åˆ
        memory_weights = torch.softmax(
            torch.randn(batch_size, self.num_maskmem, device=curr_vision_feats.device),
            dim=1
        )
        
        weighted_memory = torch.sum(
            self.memory_bank * memory_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1),
            dim=1
        )
        
        memory_features = curr_vision_feats + 0.1 * weighted_memory
        
        return memory_features

class SAMURAIStatelessONNX(nn.Module):
    """
    æ— çŠ¶æ€çš„SAMURAI ONNXæ¨¡å‹
    æ¯æ¬¡è°ƒç”¨éƒ½éœ€è¦æä¾›å®Œæ•´çš„è¾“å…¥ï¼ŒåŒ…æ‹¬å†…å­˜é“¶è¡Œ
    """
    
    def __init__(self, predictor):
        super().__init__()
        
        self.image_encoder = predictor.image_encoder
        self.prompt_encoder = predictor.sam_prompt_encoder
        self.mask_decoder = predictor.sam_mask_decoder
        self.memory_encoder = predictor.memory_encoder
        
        self.image_size = predictor.image_size
        
    def forward(self,
                image: torch.Tensor,
                point_coords: torch.Tensor,
                point_labels: torch.Tensor,
                memory_bank: torch.Tensor,
                box_coords: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        æ— çŠ¶æ€çš„å‰å‘ä¼ æ’­
        
        Args:
            image: [B, 3, H, W] è¾“å…¥å›¾åƒ
            point_coords: [B, N, 2] ç‚¹åæ ‡
            point_labels: [B, N] ç‚¹æ ‡ç­¾
            memory_bank: [B, num_mem, C, H, W] å†…å­˜é“¶è¡Œ
            box_coords: [B, 4] å¯é€‰çš„è¾¹ç•Œæ¡†
            
        Returns:
            masks: [B, num_masks, H, W] é¢„æµ‹æ©ç 
            iou_predictions: [B, num_masks] IoUé¢„æµ‹
            updated_memory_bank: [B, num_mem, C, H, W] æ›´æ–°çš„å†…å­˜é“¶è¡Œ
        """
        
        # å›¾åƒç¼–ç 
        if image.shape[-2:] != (self.image_size, self.image_size):
            image = torch.nn.functional.interpolate(
                image, size=(self.image_size, self.image_size),
                mode='bilinear', align_corners=False
            )
        
        backbone_out = self.image_encoder(image)
        if isinstance(backbone_out, (list, tuple)):
            image_features = backbone_out[0]
        elif isinstance(backbone_out, dict):
            image_features = list(backbone_out.values())[0]
        else:
            image_features = backbone_out
        
        # æç¤ºç¼–ç 
        points = (point_coords, point_labels)
        boxes = box_coords
        
        sparse_embeddings, dense_embeddings = self.prompt_encoder(
            points=points, boxes=boxes, masks=None
        )
        
        # æ©ç è§£ç 
        image_pe = self.prompt_encoder.get_dense_pe()
        masks, iou_predictions, _, _ = self.mask_decoder(
            image_embeddings=image_features,
            image_pe=image_pe,
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=True,
            repeat_image=False,
            high_res_features=None
        )
        
        # å†…å­˜æ›´æ–°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        batch_size = image_features.shape[0]
        output_mask = masks[:, 0:1]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ©ç 
        
        # è°ƒæ•´æ©ç å°ºå¯¸
        if output_mask.shape[-2:] != image_features.shape[-2:]:
            mask_resized = torch.nn.functional.interpolate(
                output_mask,
                size=image_features.shape[-2:],
                mode='bilinear',
                align_corners=False
            )
        else:
            mask_resized = output_mask
        
        # æ›´æ–°å†…å­˜é“¶è¡Œ
        masked_features = image_features * mask_resized
        updated_memory_bank = memory_bank.clone()
        
        # å¾ªç¯ç§»ä½å¹¶æ·»åŠ æ–°å†…å­˜
        updated_memory_bank[:, 1:] = updated_memory_bank[:, :-1]
        updated_memory_bank[:, 0] = masked_features
        
        return masks, iou_predictions, updated_memory_bank

def export_end_to_end_model():
    """å¯¼å‡ºç«¯åˆ°ç«¯æ¨¡å‹"""
    
    print("ğŸ¯ å¯¼å‡ºç«¯åˆ°ç«¯SAMURAIæ¨¡å‹")
    print("=" * 40)
    
    # åŠ è½½æ¨¡å‹
    model_path = os.path.join(project_root, "sam2/checkpoints/sam2.1_hiera_base_plus.pt")
    config_path = os.path.join(project_root, "sam2/sam2/configs/samurai/sam2.1_hiera_b+.yaml")
    
    print(f"åŠ è½½æ¨¡å‹: {model_path}")
    predictor = build_sam2_video_predictor(config_path, model_path, device="cpu")
    predictor.eval()
    
    # åˆ›å»ºç«¯åˆ°ç«¯æ¨¡å‹
    end_to_end_model = SAMURAIEndToEndONNX(predictor)
    end_to_end_model.eval()
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 1
    image_size = predictor.image_size
    
    image = torch.randn(batch_size, 3, image_size, image_size, dtype=torch.float32)
    point_coords = torch.tensor([[[512.0, 512.0], [256.0, 256.0]]], dtype=torch.float32)
    point_labels = torch.tensor([[1, 0]], dtype=torch.int64)
    box_coords = torch.tensor([[100.0, 100.0, 400.0, 400.0]], dtype=torch.float32)
    
    print(f"æµ‹è¯•è¾“å…¥:")
    print(f"  å›¾åƒ: {image.shape}")
    print(f"  ç‚¹åæ ‡: {point_coords.shape}")
    print(f"  ç‚¹æ ‡ç­¾: {point_labels.shape}")
    print(f"  è¾¹ç•Œæ¡†: {box_coords.shape}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    with torch.no_grad():
        try:
            masks, iou_predictions, memory_features = end_to_end_model(
                image, point_coords, point_labels, box_coords
            )
            print(f"\nè¾“å‡º:")
            print(f"  æ©ç : {masks.shape}")
            print(f"  IoUé¢„æµ‹: {iou_predictions.shape}")
            print(f"  å†…å­˜ç‰¹å¾: {memory_features.shape}")
            
        except Exception as e:
            print(f"å‰å‘ä¼ æ’­å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    # å¯¼å‡ºONNX
    output_dir = "onnx_models"
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, "samurai_end_to_end.onnx")
    
    print(f"\nå¯¼å‡ºåˆ°: {output_path}")
    
    try:
        torch.onnx.export(
            end_to_end_model,
            (image, point_coords, point_labels, box_coords),
            output_path,
            input_names=["image", "point_coords", "point_labels", "box_coords"],
            output_names=["masks", "iou_predictions", "memory_features"],
            opset_version=17,
            do_constant_folding=True,
            export_params=True,
            verbose=True
        )
        
        print("âœ… ç«¯åˆ°ç«¯æ¨¡å‹å¯¼å‡ºæˆåŠŸ!")
        return True
        
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def export_stateless_model():
    """å¯¼å‡ºæ— çŠ¶æ€æ¨¡å‹"""
    
    print("\nğŸ”„ å¯¼å‡ºæ— çŠ¶æ€SAMURAIæ¨¡å‹")
    print("=" * 45)
    
    # åŠ è½½æ¨¡å‹
    model_path = os.path.join(project_root, "sam2/checkpoints/sam2.1_hiera_base_plus.pt")
    config_path = os.path.join(project_root, "sam2/sam2/configs/samurai/sam2.1_hiera_b+.yaml")
    
    predictor = build_sam2_video_predictor(config_path, model_path, device="cpu")
    predictor.eval()
    
    # åˆ›å»ºæ— çŠ¶æ€æ¨¡å‹
    stateless_model = SAMURAIStatelessONNX(predictor)
    stateless_model.eval()
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 1
    image_size = predictor.image_size
    
    image = torch.randn(batch_size, 3, image_size, image_size, dtype=torch.float32)
    point_coords = torch.tensor([[[512.0, 512.0]]], dtype=torch.float32)
    point_labels = torch.tensor([[1]], dtype=torch.int64)
    memory_bank = torch.randn(batch_size, 7, 256, 64, 64, dtype=torch.float32)
    box_coords = torch.tensor([[100.0, 100.0, 400.0, 400.0]], dtype=torch.float32)
    
    print(f"æ— çŠ¶æ€æ¨¡å‹è¾“å…¥:")
    print(f"  å›¾åƒ: {image.shape}")
    print(f"  ç‚¹åæ ‡: {point_coords.shape}")
    print(f"  ç‚¹æ ‡ç­¾: {point_labels.shape}")
    print(f"  å†…å­˜é“¶è¡Œ: {memory_bank.shape}")
    print(f"  è¾¹ç•Œæ¡†: {box_coords.shape}")
    
    # æµ‹è¯•
    with torch.no_grad():
        try:
            masks, iou_predictions, updated_memory_bank = stateless_model(
                image, point_coords, point_labels, memory_bank, box_coords
            )
            print(f"  æ©ç : {masks.shape}")
            print(f"  IoUé¢„æµ‹: {iou_predictions.shape}")
            print(f"  æ›´æ–°çš„å†…å­˜é“¶è¡Œ: {updated_memory_bank.shape}")
            
        except Exception as e:
            print(f"æ— çŠ¶æ€æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    # å¯¼å‡º
    output_path = os.path.join("onnx_models", "samurai_stateless.onnx")
    
    try:
        torch.onnx.export(
            stateless_model,
            (image, point_coords, point_labels, memory_bank, box_coords),
            output_path,
            input_names=["image", "point_coords", "point_labels", "memory_bank", "box_coords"],
            output_names=["masks", "iou_predictions", "updated_memory_bank"],
            opset_version=17,
            do_constant_folding=True,
            export_params=True,
            verbose=False
        )
        
        print(f"âœ… æ— çŠ¶æ€æ¨¡å‹å¯¼å‡ºæˆåŠŸ: {output_path}")
        return True
        
    except Exception as e:
        print(f"âŒ æ— çŠ¶æ€æ¨¡å‹å¯¼å‡ºå¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ SAMURAI ç«¯åˆ°ç«¯ ONNX å¯¼å‡º")
    print("=" * 50)
    
    success1 = export_end_to_end_model()
    success2 = export_stateless_model()
    
    print("\n" + "=" * 50)
    if success1 or success2:
        print("ğŸ‰ è‡³å°‘ä¸€ä¸ªç«¯åˆ°ç«¯æ¨¡å‹å¯¼å‡ºæˆåŠŸ!")
        
        if success1:
            print("âœ… ç«¯åˆ°ç«¯æ¨¡å‹: onnx_models/samurai_end_to_end.onnx")
        if success2:
            print("âœ… æ— çŠ¶æ€æ¨¡å‹: onnx_models/samurai_stateless.onnx")
    else:
        print("âŒ æ‰€æœ‰å¯¼å‡ºéƒ½å¤±è´¥äº†")

if __name__ == "__main__":
    main()
